---
title: "AAHW3 - Kilbourn"
author: "CJK"
date: "2023-06-26"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Homework 3 - R Portion

This is in response to question 2. The prompt was:
2.	Build an optimal Lasso, Ridge, or ElasticNet logistic regression model using the `attrition` data we used for class that includes a “penalty” and “mixture” parameter. I’ll leave it up to you to determine what “optimal” means. 


My first step is to load the attrition data set (note that because this is under a different project in order to connect it to github, I will need to import the dataset again so it's compatible with git and with this markdown file, even though I have already imported this dataset during the session three lab).
```{r}
library(readxl)
WA_Fn_UseC_HR_Employee_Attrition <- read_excel("WA_Fn-UseC_-HR-Employee-Attrition.xlsx")
View(WA_Fn_UseC_HR_Employee_Attrition)
```

Also, the name of the dataset is crazy long, so I will shorten it to just data for now.
```{r}
data <- WA_Fn_UseC_HR_Employee_Attrition
```

I will call the tidyverse package - I may need other packages later on, but this one will be sufficient to get me started.
```{r}
library(tidyverse)
```

And I have already dome some exploratory 'poking' at this data during the lab, but I will run a couple of functions just to refamiliarize myself with what I'm working with. First, I will glimpse the data.
```{r}
glimpse(data)
```
Then I'll get a better look at the column names.
```{r}
colnames(data)
```

And I remember there being no missing data, but I will double check this just to be certain.
```{r}
apply(is.na(data), 2, sum)
```

So there's no missing data, and there are 35 total variables. I will now go into the regularization (shrinking) methods: ridge regression, lasso, and elasticnet. The concept behind regularization is to shrink the coefficient estimates of predictor variables. This is useful if a model has a large number of predictors (and we're not sure which ones are useful and which ones are not 'pulling their weight') since a model will generally perform 'better' as the number of predictors increases, regardless of the usefulness of any and all predictors. So if a model has 100 variables but only 8 are responsible for the majority of effective performance of the model, regularization methods will show a significant drop-off in the decrease of effectiveness in the non-useful 92 variables. It is important to remember that the goal is parsimony, wanting to find the best performing model with strong predictive power and good fit that also includes the lowest quantity of predictor variables.
The methods of ridge, lasso, and elasticnet vary slightly in their regularization processes, but they all shrink the coefficients of the predictors. Ridge shrinks toward 0 but never gets to 0, lasso can shrink coefficients to 0, and elasticnet combines elements of both ridge and lasso.

And 35 is a pretty hefty number of predictors, (especially when my goal is to be able to tell key business leaders what ‘levers’ to pull to have an influence on attrition), so I will now explore which methods of regularization provide us with the best fitting model to move forward with.

I will experiment with a ridge regression first. I did some research on a few blogs, and I did find that ridge is not recommended for logistic models, however, I will proceed with running a ridge just to get the experience with it (and see, comparatively, how it performs with other regularization processes).

The way to create a ridge regression model specification is via the linear_reg() function and setting the mixture to 0 (this will be very similar to lasso except the mixture will be set to 1, and the elasticnet in which the number will be between 0 and 1 indicating a blend of ridge and lasso).
For now, my penalty is set to 0 because I don't have any knowledge or information at this time to make a more informed selection - that will come later.

I will also call the glmnet and parsnip packages.
Note that I adjusted the code to reflect that this is a logistic regression (logistic_reg) and that my mode should be set to classification. I also will set my seed here. I'm not sure if I need to with regularization, but it should help me with reproducability of my results if I need that).
```{r}
library(parsnip)
library(glmnet)

set.seed(31)
ridge_spec <- logistic_reg(mixture = 0, penalty = 0) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```

I will now fit the ridge regression to my data. Just to remind myself, I am currently looking at Attrition as my outcome variable (binary outcome of 'yes' or 'no') and all other variables as my predictors.
```{r, eval=FALSE}
ridge_fit <- fit(ridge_spec, Attrition ~ ., data = data)
```
So I learned something! In the code above, the error stated that 'contrasts can be applied only to factors with 2 or more levels. I remember from exploring this data before that some of the predictors had the exact same responses across all respondents. This may be the problem. I will look at which of these factors fit this description and create a new dataframe with the remaining predictors only.

First, I will look at character (non-qualitative) variables.
```{r}
library(conflicted)
conflict_prefer("filter", "dplyr")

data %>%
    select_if(is.character) %>%
    map(unique)
```

The Over18 variable is a problem - all responses are the same, so there is no variance. 

Now I will look at numeric variables
```{r}
data %>%
    select_if(is.numeric) %>%
    map(~ unique(.) %>% length())
```

EmployeeCount and StandardHours are also problematic. Once more, there is no variance since responses are the same across all respondents.



There are a couple different methods for handling these three variables. Because ridge regression is new to me, I'm not sure if there is a preferred handling method (I experimented with creating a recipe using the step_zv() function, but I wasn't sure if that was overcomplicating things). So for now, I used the select function to create a new dataframe called newdata that does not include the three non-variance variables.
```{r}
newdata <- data %>% select(-Over18,-EmployeeCount,-StandardHours)
```

I also discovered through trial and error (didn't leave all the error and wrong codes because they would be a mile long!) that something needed to be adjusted for my outcome variable as well. I received some words of wisdom that my outcome variable needed to be set as a factor instead of a character, so I will do this now.
```{r}
newdata$Attrition <- factor(newdata$Attrition)
```

And I will confirm that it is, indeed, a character now.
```{r}
is.factor(newdata$Attrition)
```


And now, with my revised dataset, I will try to fit my ridge regression.
```{r}
ridge_fit <- fit(ridge_spec, Attrition ~ ., data = newdata)
```

That worked! Now I will use the tidy function to look at the estimates for each variable. Note that the penalty is at 0 for all of them because I haven't tuned the penalty yet (since I don't have the info yet to know what it should be tuned to).
```{r}
tidy(ridge_fit)
```


And I will plot the variables. If the ridge specification worked, all variables should trend toward 0.
```{r}
plot(ridge_fit$fit, xvar = "lambda")
```
Good, all coefficients trend toward 0. This is what ridge regression was designed to do, so that's what I want to see.


Now I will split the data into my training set and my test set. I will use the strata argument to split my data stratified based on Attrition, since it is an unbalanced sample (far more people staying than quitting). I will also set up my data splits for my v-fold cross validation here.
I will call the tidymodels package for the splitting and cross-validation fold-making process.
```{r}
library(tidymodels)
data_split <- initial_split(newdata, strata = "Attrition")
data_train <- training(data_split)
data_test <- testing(data_split)
data_fold <- vfold_cv(data_train, v = 10)
```

Now I can create a recipe for my ridge regression and ensure that all necessary preprocessing for my training data is taking place, including building dummy variables, normalizing predictors, etc.
I will also include the step_zv (zero variance) step, even though I am working with the dataset that doens't include the three variables with no variance. It is possible that I chould have split by data using the initial dataset and run this recipe and everything would work out from there, but for now I will leave this as is and ask about this later on.
```{r}
ridge_recipe <- 
  recipe(formula = Attrition ~ ., data = data_train) %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

And I will adjust my ridge specification to now be set to 'penalty = tune()' so the tune grid knows the penalty parameter should be tuned.
```{r}
ridge_spec <- logistic_reg(mixture = 0, penalty = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```

And I can create my workflow, adding my recipe and model set to ridge.
```{r}
ridge_workflow <- workflow() %>% 
  add_recipe(ridge_recipe) %>% 
  add_model(ridge_spec)
```

I will now create a penalty grid to determine the values of the penalty. The code below will create a grid of evenly spaced parameter values. Since I don't have any prior information about what values will work, I will set the levels to 50 to cast a wider net for this go-around.
```{r}
penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)
penalty_grid
```

That gives me what I need to fit the model to everything I have specified so far. This is my v-fold cross-validation in action.
```{r}
tune_res <- tune_grid(
  ridge_workflow,
  resamples = data_fold, 
  grid = penalty_grid
)
tune_res
```
I will visualize this more easily with the autoplot() function. 
```{r}
autoplot(tune_res)
```
One interesting trend to note is that both accuracy and my ROC area under the curve (AUC) actually decrease as the amount of regularization increases. I want both accuracy and AUC to be as high as possible. This could possibly be due to the fact that ridge regression is not recommended for logistic models, or it could be due to other factors like the regularization increasing bias and/or information loss from the coefficients approaching 0.

Regardless, this helps me visualize the process of regularization, and how changing the value for my penalty will change the metrics of my model. I will also expect the optimal penalty to be quite low (since the higher penalty decreases both accuracy and AUC).

I can collect raw data about these metrics here. 
```{r}
collect_metrics(tune_res)
```
The same pattern emerges (both accuracy and AUC decreasing with a higher penalty).


Knowing that ridge is probably not what I will ultimately choose, I can still select the optimal penalty (it will probably be quite low) for ridge. I can do this according to either of the two metrics: accuracy or AUC. I chose AUC, primarily because accuracy does not always give the most comprehensive picture of a model's predictive accuracy, specfically in very imbalanced datasets, which is what I'm dealing with (since significantly more people stay in the organization than leave the organization).
```{r}
best_penalty_auc <- select_best(tune_res, metric = "roc_auc")
best_penalty_auc
```

I will now apply this best-fit penalty to my workflow.
```{r}
ridge_final <- finalize_workflow(ridge_workflow, best_penalty_auc)
ridge_final_fit <- fit(ridge_final, data = data_train)
```

And I will see how well this performs with my testing set.
```{r}
augment(ridge_final_fit, new_data = data_test) %>%
    roc_auc(truth = Attrition, .pred_Yes, event_level = "second")
```

After some trial and error with the augment code (I found that the second line of the code needed to be configured differently for the logistic model), I got this to work. I am not sure if I can fully trust the results, but the estimate I got was 0.811. I will compare this to lasso and see how things look there.
So in a nutshell, my ridge regression has a mixture of 0, penalty of 0.00001, and a predictive performance metric of 0.811 on my test data. 


Now I will see if the lasso can improve this performance.

My code for lasso will look quite similar to ridge, except that to specify lasso, I will set my mixture to 1 instead of 0 (1 denotes lasso, 0 denotes ridge, and anything between 0 and 1 denotes an elasticnet).

First things first, I will build my recipe. Once more, I want to normalize my predictors, ensure the setup of dummy variables, etc.
```{r}
lasso_recipe <- 
  recipe(formula = Attrition ~ ., data = data_train) %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

Now I will create my lasso specification.
```{r}
lasso_spec <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet") 
lasso_workflow <- workflow() %>% 
  add_recipe(lasso_recipe) %>% 
  add_model(lasso_spec)
```

And I will set up my penalty grid to determine what the penalty value should be.
```{r}
penalty_grid <- grid_regular(penalty(range = c(-2, 2)), levels = 50)
```

I will now tune and check out the autoplot to visualize the results.
```{r}
tune_res <- tune_grid(
  lasso_workflow,
  resamples = data_fold, 
  grid = penalty_grid
)
autoplot(tune_res)
```
Interestingly, the AUC looks quite similar to that of the ridge, but accuracy does seem to drop off much sooner in the lasso than it did in ridge. Currently, I am using AUC as my metric to focus on, so this may not influence the performance as much. Once more, AUC will likely make the most sense as the metric since I'm dealing with an imbalanced dataset.

Now I will make sure the tuning parameter is tuning my penalty based on the most optimal AUC metric.
```{r}
best_penalty_auc <- select_best(tune_res, metric = "roc_auc")
best_penalty_auc
```

And I will finalize fitting my lasso with the tuned penalty to my workflow.
```{r}
lasso_final <- finalize_workflow(lasso_workflow, best_penalty_auc)
lasso_final_fit <- fit(lasso_final, data = data_train)
```

And I can see how well the lasso specification performs.
```{r}
augment(lasso_final_fit, new_data = data_test) %>%
    roc_auc(truth = Attrition, .pred_Yes, event_level = "second")
```

I got a value of 0.796, which is is actually slightly worse performance-wise than the ridge. Very interesting! I will hope that an elasticnet specification will help improve performance more.
So my results from lasso are: mixture of 1, penalty of 0.01, predictive performance of 0.796.

Now last but not least, I will take a look at elasticnet, which is a blend of ridge and lasso. Based on what I've read, elasticnet is a suitable option for logistic.

The steps will look quite similar to ridge and lasso, but my mixture (in the code below this one) will not be 0 or 1. For now, I will set it to .5 to see how that performs.
```{r}
elnet_recipe <- 
  recipe(formula = Attrition ~ ., data = data_train) %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

This will be my elasticnet workflow.
```{r}
elnet_spec <- 
  logistic_reg(penalty = tune(), mixture = .5) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet") 
elnet_workflow <- workflow() %>% 
  add_recipe(elnet_recipe) %>% 
  add_model(elnet_spec)
```


And my penalty grid once more. It's possible I can adjust this to less than 50 levels, but I will leave it at 50 for now to maintain consistency.
```{r}
penalty_grid <- grid_regular(penalty(range = c(-2, 2)), levels = 50)
```

And I will tune my grid and look at the autoplot.
```{r}
tune_res <- tune_grid(
  elnet_workflow,
  resamples = data_fold, 
  grid = penalty_grid
)
autoplot(tune_res)
```

Now once more, I will use AUC to determine the optimal penalty for my elasticnet specification.
```{r}
best_penalty_auc <- select_best(tune_res, metric = "roc_auc")
best_penalty_auc
```

And I will finalize fitting my elasticnet with the tuned penalty to my workflow.
```{r}
elnet_final <- finalize_workflow(elnet_workflow, best_penalty_auc)
elnet_final_fit <- fit(elnet_final, data = data_train)
```

And I can see how well the elasticnet specification performs.
```{r}
augment(elnet_final_fit, new_data = data_test) %>%
    roc_auc(truth = Attrition, .pred_Yes, event_level = "second")
```

The result is 0.807. Slightly better than lasso, slightly worse than ridge.

I have copied the code above and will run through a few iterative processes, adjusting the value set for the mixture, just to see how this shifts the results (for the sake space, I will only leave up the code that correponds to the best fitting mixture value, but I will note the values that I tried below).
```{r}
elnet_spec <- 
  logistic_reg(penalty = tune(), mixture = .05) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet") 
elnet_workflow <- workflow() %>% 
  add_recipe(elnet_recipe) %>% 
  add_model(elnet_spec)
```

```{r}
penalty_grid <- grid_regular(penalty(range = c(-2, 2)), levels = 50)
```

```{r}
tune_res <- tune_grid(
  elnet_workflow,
  resamples = data_fold, 
  grid = penalty_grid
)
autoplot(tune_res)
```

```{r}
best_penalty_auc <- select_best(tune_res, metric = "roc_auc")
best_penalty_auc
```

```{r}
elnet_final <- finalize_workflow(elnet_workflow, best_penalty_auc)
elnet_final_fit <- fit(elnet_final, data = data_train)
```

```{r}
augment(elnet_final_fit, new_data = data_test) %>%
    roc_auc(truth = Attrition, .pred_Yes, event_level = "second")
```

mixture = 0.65; penalty = 0.01; estimate (performance) = 0.806
mixture = 0.4; penalty = 0.012; estimate (performance) = 0.806
mixture = 0.25; penalty = 0.01; estimate (performance) = 0.810
mixture = 0.05; penalty = 0.01; estimate (performance) = 0.811


It was interesting to see how adjusting the value for the mixture would change the results (but only slightly). I also noted that the grids would produce somehwat different results as well - notably that there was an uptick in accuracy for some of the above specifications, including the chosen specificaion of a mixture of 0.05.

Based on all of the above information gleaned from looking at ridge, lasso, and elasticnet specifications, I have deemed that elasticnet is the optimal option for regularization for my logistic model. Although the performance was comparable for elasticnet and ridge, I have chosen elasticnet as a better option since ridge is not recommended for classification of a binary outcome variable (logistic). 

My final results for optimal regularization of my data are as follows:
Elasticnet
Mixture of 0.05
Penalty of 0.01
Performance of 0.811


Also, I did some sleuthing to see if there was a method for determining which variables would be removed for 'underperforming' in either a lasso or elasticnet specification, but I was not able to get any of the code to work. I will note this as a question to ask later, and I will continue exploring potential options to find these results in the meantime.

